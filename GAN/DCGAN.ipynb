{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rashmi0888/Deep-Learning/blob/master/GAN/DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUeASTWfNRuE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "bad5da9d-1d95-4538-977c-4daf089c9d01"
      },
      "source": [
        "# Import relevant components\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Input, Dropout, BatchNormalization, Reshape, Conv2D, UpSampling2D\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers.core import Dense, Flatten, Activation\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.datasets import mnist\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "np.random.seed(100)\n",
        "K.set_image_dim_ordering('th')\n",
        "\n",
        "# Data Dimensions\n",
        "noise_dim = 100\n",
        "epochs = 100\n",
        "\n",
        "\n",
        "# Download, Reshape and Normalize Data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
        "X_train = X_train[:, np.newaxis, :, :]\n",
        "\n",
        "\n",
        "# Optimiser\n",
        "adam = Adam(lr=0.0001, beta_1=0.5)\n",
        "\n",
        "\n",
        "# Create Generator\n",
        "generator = Sequential()\n",
        "generator.add(Dense(128*7*7, input_dim=noise_dim))\n",
        "generator.add(LeakyReLU(0.2))\n",
        "generator.add(Reshape((128, 7, 7)))\n",
        "generator.add(UpSampling2D(size=(2, 2)))\n",
        "generator.add(Conv2D(64, kernel_size=(5, 5), padding='same'))\n",
        "generator.add(LeakyReLU(0.2))\n",
        "generator.add(UpSampling2D(size=(2, 2)))\n",
        "generator.add(Conv2D(1, kernel_size=(5, 5), padding='same', activation='tanh'))\n",
        "generator.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Create Discriminator\n",
        "discriminator = Sequential()\n",
        "discriminator.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='same', input_shape=(1, 28, 28)))\n",
        "discriminator.add(LeakyReLU(0.2))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Conv2D(128, kernel_size=(5, 5), strides=(2, 2), padding='same'))\n",
        "discriminator.add(LeakyReLU(0.2))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Flatten())\n",
        "discriminator.add(Dense(1, activation='sigmoid'))\n",
        "discriminator.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Create GAN by combining generator and discriminator\n",
        "discriminator.trainable = False\n",
        "input = Input(shape=(noise_dim,))\n",
        "hidden = generator(input)\n",
        "output = discriminator(hidden)\n",
        "gan = Model(input, output)\n",
        "gan.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Plot loss and accuracy\n",
        "def plot_losses(losses, learning_rate, batch_size):\n",
        "    discriminator_loss = [v[0] for v in losses[\"D\"]]\n",
        "    generator_loss = [v[0] for v in losses[\"G\"]]\n",
        "    discriminator_accuracy = [v[1] for v in losses[\"D\"]]\n",
        "    generator_accuracy = [v[1] for v in losses[\"G\"]]\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(discriminator_loss, label=\"Discriminator loss\")\n",
        "    plt.plot(generator_loss, label=\"Generator loss\")\n",
        "    plt.title(\"LR:{}, BS:{},\".format(learning_rate, batch_size), pad=-20)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('Loss_DCGAN.png')\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(discriminator_accuracy, label=\"Discriminator accuracy\")\n",
        "    plt.plot(generator_accuracy, label=\"Generator accuracy\")\n",
        "    plt.title(\"LR:{}, BS:{},\".format(learning_rate, batch_size), pad=-20)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.savefig('Accuracy_DCGAN.png')\n",
        "\n",
        "\n",
        "# Plot generated images\n",
        "def plot_generated_images(epoch, examples=10, dim=(1, 10), figsize=(12, 2)):\n",
        "    noise = np.random.normal(0, 1, size=(examples, noise_dim))\n",
        "    generated_images = generator.predict(noise)\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i + 1)\n",
        "        plt.imshow(generated_images[i,0], interpolation='nearest', cmap='gray_r')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('dcgan_generated_image_epoch_%d.png' % epoch)\n",
        "\n",
        "\n",
        "# Train GAN\n",
        "def train(learning_rate = 0.0002, batch_size = 128, plot_freq=10):\n",
        "    batch_count = int(X_train.shape[0] /batch_size)\n",
        "    losses = {\"D\": [], \"G\": []}\n",
        "    for e in range(1, epochs + 1):\n",
        "        print('\\n')\n",
        "        print('-' * 15, 'Epoch %d' % e, '-' * 15)\n",
        "        d_loss = None\n",
        "        g_loss = None\n",
        "        for _ in tqdm(range(batch_count)):\n",
        "            # Get a random set of real images from training set\n",
        "            image_batch = X_train[np.random.randint(0, X_train.shape[0], size=batch_size)]\n",
        "\n",
        "            # Generate random noise as input to initialize generator\n",
        "            noise = np.random.normal(0, 1, size=(batch_size, noise_dim))\n",
        "\n",
        "            # Generate fake MNIST images from noised input\n",
        "            generated_images = generator.predict(noise)\n",
        "\n",
        "            # Create different batches of real and fake images\n",
        "            X = np.concatenate((image_batch, generated_images))\n",
        "\n",
        "            # Labels for real and fake images\n",
        "            y = np.zeros(2 * batch_size)\n",
        "            y[:batch_size] = 0.9  # One-sided label smoothing\n",
        "\n",
        "            # Pre-train discriminator on real and fake images before starting GAN\n",
        "            discriminator.trainable = True\n",
        "            d_loss = discriminator.train_on_batch(X, y)\n",
        "\n",
        "            # Trick noised input of generator as real images\n",
        "            noise = np.random.normal(0, 1, size=(batch_size, noise_dim))\n",
        "            y2 = np.ones(batch_size)\n",
        "\n",
        "            # During training of GAN, weights of discriminator should be fixed.\n",
        "            # We can enforce that by setting trainable flag\n",
        "            discriminator.trainable = False\n",
        "\n",
        "            #Train GAN\n",
        "            g_loss = gan.train_on_batch(noise, y2)\n",
        "\n",
        "        # Store loss from final batch of this epoch\n",
        "        losses[\"D\"].append(d_loss)\n",
        "        losses[\"G\"].append(g_loss)\n",
        "\n",
        "        # Plot generated images for selected epochs\n",
        "        if e == 1 or e==epochs or e % plot_freq == 0:\n",
        "            plot_generated_images(e)\n",
        "\n",
        "    # Plot losses\n",
        "    plot_losses(losses, learning_rate, batch_size)\n",
        "\n",
        "\n",
        "train(learning_rate = 0.0001, batch_size = 256, plot_freq = 10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "--------------- Epoch 1 ---------------\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|â–         | 5/234 [00:02<04:17,  1.13s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2AB0ydnNXID"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}